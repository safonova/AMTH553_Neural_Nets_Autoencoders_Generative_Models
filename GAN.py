

## Please see amth553_4.2_run_GANs.ipynb for my GAN training and analysis code.

# Sasha Safonova

"""import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torch.optim as optim
import torchvision.datasets as datasets
# import imageio
import numpy as np
import matplotlib
from torchvision.utils import make_grid, save_image
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
# from tqdm import tqdm
matplotlib.style.use('ggplot')

class Generator(nn.Module):
    def __init__(self, nz):
        super(Generator, self).__init__()
        self.nz = nz # the dimension of the random noise used to seed the Generator
        self.main = nn.Sequential( # nn.sequential is a handy way of combining multiple layers.
            nn.Linear(self.nz, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 784),
            nn.Tanh(),
        )
    def forward(self, x):
        return self.main(x).view(-1, 1, 28, 28)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.n_input = 784
        self.main = nn.Sequential(
            nn.Linear(self.n_input, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )
    def forward(self, x):
        x = x.view(-1, 784)
        return self.main(x)


def train_discriminator(optimizer, real_data, fake_data, model_discriminator=None):
    """
    Train the discriminator on a minibatch of data.
    INPUTS
        :param optimizer: the optimizer used for training
        :param real_data: the batch of training data
        :param fake_data: the data generated by the generator from random noise
    The discriminator will incur two losses: one from trying to classify the real data, and another from classifying the fake data.
    TODO: Fill in this function.
    It should
    1. Run the discriminator on the real_data and the fake_data
    2. Compute and sum the respective loss terms (described in the assignment)
    3. Backpropogate the loss (e.g. loss.backward()), and perform optimization (e.g. optimizer.step()).
    """
    # model_distriminator = Discriminator()
    for ii, real_sample in enumerate(real_data):
        fake_sample = fake_data[ii]

        fake_output = model_discriminator(fake_sample)
        real_output = model_discriminator(real_sample)

        BCEloss_fn_fake = nn.BCELoss()
        BCEloss_fn_real = nn.BCELoss()
        fake_loss = BCEloss_fn_fake(fake_output, fake_sample)
        real_loss = BCEloss_fn_real(real_output, real_sample)
        loss = fake_loss + real_loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    # we'll return the loss for book-keeping purposes. (E.g. if you want to make plots of the loss.)
    return loss

def train_generator(optimizer, fake_data, model_generator=None):
    """
    Performs a single training step on the generator.
    :param optimizer: the optimizer
    :param fake_data: forgeries, created by the generator from random noise. (Done before calling this function.)
    :return:  the generator's loss
    TODO: Fill in this function
    It should
    1. Run the discriminator on the fake_data
    2. compute the resultant loss for the generator (as described in the assignment)
    3. Backpropagate the loss, and perform optimization
    """

    # model_generator = Generator()
    for ii, fake_sample in enumerate(fake_data):
        fake_output = model_generator(fake_sample)
        BCEloss_fn_fake = nn.BCELoss()
        loss = BCEloss_fn_fake(fake_output, fake_sample)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return loss


def main():
    # import data
    batch_size = 100
    train_data = datasets.MNIST(
        root='../data',
        train=True,
        download=True,
        transform=transforms.ToTensor()
    )
    """prelim_train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

    for data in prelim_train_loader:
        # perform training
        data_shape = len(data)
        break"""
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

    num_epochs = 1000
    nz = 3 # dimension of random noise
    generator = Generator(nz)
    discriminator = Discriminator()#nz)
    G_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)
    D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)

    #TODO: Build a training loop for the GAN
    # For each epoch, you'll
    # 1. Loop through the training data. For each batch, feed random noise into the generator
    # to generate fake_data of the corresponding size.
    # 2. Feed the fake data and real data into the train_discriminator and train_generator functions
    # At the end of each epoch, use the below functions to save a grid of generated images.

    for epoch in range(num_epochs):
        for data, targets in train_loader:
            # perform training
            data_shape = data[0].shape
            noise = torch.rand(nz)
            fake_data = generator(noise)
            discr_loss = train_discriminator(D_optimizer, data, fake_data, model_discriminator=discriminator)
            gen_loss = train_generator(G_optimizer, noise, model_generator=generator)
        generated_img = generator(noise)
        # reshape the image tensors into a grid
        generated_img = make_grid(generated_img)
        # save the generated torch tensor images
        save_image(generated_img, f"../outputs/gen_img{epoch}.png")

if __name__=="__main__":
    main()"""